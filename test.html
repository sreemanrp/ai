<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Offline AI Agent</title>
    <style>
        body {
            margin: 0;
            display: flex;
            justify-content: center;
            align-items: center;
            height: 100vh;
            background-color: black;
            overflow: hidden;
            flex-direction: column;
            backdrop-filter: blur(30px); /* Full-screen blur effect */
        }

        .dot {
            width: 100px;
            height: 100px;
            background-color: red;
            border-radius: 50%;
            transition: opacity 0.1s ease, filter 0.3s ease;
            filter: blur(60px); /* Ensuring full blur */
            box-shadow: 0 0 60px red; /* Stronger glow to avoid exposure */
            opacity: 1;
        }

        .dot.blink {
            animation: blink 0.3s infinite alternate;
        }

        @keyframes blink {
            to {
                opacity: 0.5;
            }
        }

        .error-message {
            display: none;
            position: absolute;
            bottom: 20px;
            left: 20px;
            background-color: #4c0000;
            color: #ff7777;
            padding: 10px 20px;
            border-radius: 10px;
            font-size: 16px;
            font-weight: bold;
            box-shadow: 0 0 10px #ff4444;
            border: 1px solid #ff4444;
        }
    </style>
</head>
<body>
    <div class="dot"></div>
    <div class="error-message">⚠️ Loud noise detected</div>

    <script>
        const dot = document.querySelector('.dot');
        const errorMessage = document.querySelector('.error-message');

        let recognition;
        let isSpeaking = false;
        let audioContext, analyser, microphone;

        async function startVoiceDetection() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                analyser = audioContext.createAnalyser();
                microphone = audioContext.createMediaStreamSource(stream);
                microphone.connect(analyser);

                analyser.fftSize = 256;
                const bufferLength = analyser.frequencyBinCount;
                const dataArray = new Uint8Array(bufferLength);

                function analyzeNoise() {
                    analyser.getByteFrequencyData(dataArray);
                    const average = dataArray.reduce((a, b) => a + b) / bufferLength;

                    if (average > 100) {
                        errorMessage.style.display = "block";
                        setTimeout(() => errorMessage.style.display = "none", 2000);
                    }

                    requestAnimationFrame(analyzeNoise);
                }

                analyzeNoise();
            } catch (error) {
                console.error("Microphone access error:", error);
            }
        }

        function startSpeechRecognition() {
            try {
                recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
                recognition.continuous = true;
                recognition.lang = "en-US";

                recognition.onresult = (event) => {
                    const transcript = event.results[event.results.length - 1][0].transcript.toLowerCase();
                    console.log("User said:", transcript);

                    dot.classList.add('blink');
                    clearTimeout(blinkTimeout);
                    var blinkTimeout = setTimeout(() => dot.classList.remove('blink'), 1000);

                    respondToUser(transcript);
                };

                recognition.onerror = (event) => {
                    console.error("Speech recognition error:", event.error);
                };

                recognition.start();
            } catch (error) {
                console.error("Speech recognition not supported:", error);
            }
        }

        function respondToUser(text) {
            let response = generateEcoResponse(text);
            console.log("AI Response:", response);
            speakResponse(response);
        }

        function generateEcoResponse(text) {
            if (text.includes("hello")) return "Hello! Did you know planting trees helps absorb carbon dioxide?";
            if (text.includes("climate change")) return "We can slow climate change by reducing waste and using clean energy!";
            if (text.includes("energy")) return "Switching to solar power can greatly reduce carbon emissions!";
            if (text.includes("plastic")) return "Recycling plastic helps protect our oceans and wildlife!";
            return "Together, we can make the world a greener place!";
        }

        function speakResponse(text) {
            if ('speechSynthesis' in window) {
                let utterance = new SpeechSynthesisUtterance(text);
                utterance.lang = "en-US";
                utterance.rate = 1;
                utterance.onstart = () => isSpeaking = true;
                utterance.onend = () => isSpeaking = false;
                speechSynthesis.speak(utterance);
            }
        }

        startVoiceDetection();
        startSpeechRecognition();
    </script>
</body>
</html>
