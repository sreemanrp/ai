<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Offline AI Agent</title>
    <style>
        @font-face {
            font-family: 'Brass Mono Code';
            src: url('BrassMonoCode.woff2') format('woff2'); /* Ensure the font file is available */
        }

        body {
            margin: 0;
            display: flex;
            justify-content: center;
            align-items: center;
            height: 100vh;
            background-color: black;
            overflow: hidden;
            flex-direction: column;
            font-family: 'Brass Mono Code', monospace;
            backdrop-filter: blur(30px); /* Full-screen blur effect */
        }

        .dot {
            width: 100px;
            height: 100px;
            background-color: red;
            border-radius: 50%;
            transition: opacity 0.1s ease, filter 0.3s ease;
            filter: blur(60px); /* Increased blur to prevent exposure */
            box-shadow: 0 0 80px red; /* Stronger glow to keep the dot consistently blurred */
            opacity: 1;
        }

        .dot.pulse {
            animation: pulse 1s infinite alternate;
        }

        @keyframes pulse {
            to {
                transform: scale(1.1);
                box-shadow: 0 0 100px red; /* Maintain glow intensity during pulse */
            }
        }
    </style>
</head>
<body>
    <div class="dot"></div>

    <script>
        const dot = document.querySelector('.dot');

        let recognition;
        let isSpeaking = false;
        let audioContext, analyser, microphone;
        let lastVoiceTime = 0;

        async function startVoiceDetection() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                analyser = audioContext.createAnalyser();
                microphone = audioContext.createMediaStreamSource(stream);
                microphone.connect(analyser);

                analyser.fftSize = 256;
                const bufferLength = analyser.frequencyBinCount;
                const dataArray = new Uint8Array(bufferLength);

                function analyzeNoise() {
                    analyser.getByteFrequencyData(dataArray);
                    const average = dataArray.reduce((a, b) => a + b) / bufferLength;
                    const currentTime = Date.now();

                    if (average > 30) {
                        lastVoiceTime = currentTime;
                        if (!dot.classList.contains('pulse')) {
                            dot.classList.add('pulse');
                        }
                    } else {
                        if (Date.now() - lastVoiceTime > 1000) {
                            dot.classList.remove('pulse');
                        }
                    }

                    requestAnimationFrame(analyzeNoise);
                }

                analyzeNoise();
                startSpeechRecognition();
            } catch (error) {
                console.error("Microphone access error:", error);
            }
        }

        function startSpeechRecognition() {
            try {
                recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
                recognition.continuous = true;
                recognition.lang = "en-US";

                recognition.onresult = (event) => {
                    const transcript = event.results[event.results.length - 1][0].transcript.toLowerCase();
                    console.log("User said:", transcript);

                    lastVoiceTime = Date.now();
                    if (!dot.classList.contains('pulse')) {
                        dot.classList.add('pulse');
                    }

                    respondToUser(transcript);
                };

                recognition.onerror = (event) => {
                    console.error("Speech recognition error:", event.error);
                };

                recognition.start();
            } catch (error) {
                console.error("Speech recognition not supported:", error);
            }
        }

        function respondToUser(text) {
            let response = generateEcoResponse(text);
            console.log("AI Response:", response);
            speakResponse(response);
        }

        function generateEcoResponse(text) {
            if (text.includes("hello")) return "Hello! Did you know planting trees helps absorb carbon dioxide?";
            if (text.includes("climate change")) return "We can slow climate change by reducing waste and using clean energy!";
            if (text.includes("energy")) return "Switching to solar power can greatly reduce carbon emissions!";
            if (text.includes("plastic")) return "Recycling plastic helps protect our oceans and wildlife!";
            return "Together, we can make the world a greener place!";
        }

        function speakResponse(text) {
            if ('speechSynthesis' in window) {
                let utterance = new SpeechSynthesisUtterance(text);
                utterance.lang = "en-US";
                utterance.rate = 1;
                utterance.onstart = () => isSpeaking = true;
                utterance.onend = () => {
                    isSpeaking = false;
                    if (Date.now() - lastVoiceTime > 1000) {
                        dot.classList.remove('pulse');
                    }
                };
                speechSynthesis.speak(utterance);
            }
        }

        startVoiceDetection();
    </script>
</body>
</html>
